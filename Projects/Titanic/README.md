
## Titanic
**Project Aim** - To undertake the Kaggle Titanic challenge to get a starting experience of a start-to-finish data project with cleaning, feature engineering and predictive analysis. Additionally, I will use JupyterNotebook to gain experience of using local .ipynb files and working with Github.


**Project Summary** - A straight forward project that allowed me to analyze a well known static dataset. This covered each feature to understand what they were, any missing values and the best way to handle missing values. I engineered the features Title, Age Category, Number of Family Members, Family Classification to either extract information from another feature or help to impute missing data. I also experimented with feature analysis and when it came to the predictive models used a full-set and subset of features. I built three models, Random Forest, Logistic Regression XGBoost with scikit-learn's pipelines. In the end, the pipelines saved no time but proved as a good learning experience for future projects.


**Project Outcome** - 6 Kaggle scores for the Titanic Competition that did not use the feature 'Survived' from the training set to see if individuals in the test dataset had family members that survived in the training dataset.

Full Set of Features:
- Random Forest - 0.76794
- Logistic Regression - 0.76794
- XGBoost - 0.75119

Sub-Set of Features:
- Random Forest - 0.73444
- Logistic Regression - 0.77511
- XGBoost - 0.74641


[Click Here](https://github.com/SamButterfield/TitatanicDS) to view the full repo on Github.


### [Return to the main portfolio page](/Sam_Portfolio/)

